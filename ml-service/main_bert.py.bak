from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import spacy
from sentence_transformers import SentenceTransformer, util
import re
from typing import List

app = FastAPI()

# CORS Middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Load NLP models
print("Loading spaCy model...")
try:
    nlp = spacy.load("en_core_web_sm")
except:
    print("Downloading spaCy model...")
    import os
    os.system("python -m spacy download en_core_web_sm")
    nlp = spacy.load("en_core_web_sm")

print("Loading Sentence Transformer model...")
model = SentenceTransformer('all-MiniLM-L6-v2')

# Skill database (can be expanded)
SKILLS_DATABASE = {
    "programming": ["python", "javascript", "java", "c++", "c#", "golang", "rust", "typescript", "php", "swift"],
    "ml": ["machine learning", "deep learning", "tensorflow", "pytorch", "keras", "scikit-learn", "nlp", "bert", "gpt", "neural networks"],
    "data": ["data science", "data analysis", "sql", "postgresql", "mongodb", "hadoop", "spark", "pandas", "numpy", "tableau", "powerbi"],
    "web": ["react", "angular", "vue", "django", "flask", "express", "node.js", "html", "css", "web development"],
    "cloud": ["aws", "azure", "gcp", "docker", "kubernetes", "jenkins", "devops", "ci/cd"],
    "other": ["git", "rest api", "microservices", "agile", "communication", "problem-solving", "leadership"]
}

class AnalysisRequest(BaseModel):
    resume_text: str
    job_description: str

class AnalysisResponse(BaseModel):
    ats_score: float
    match_percentage: float
    matched_skills: List[str]
    missing_skills: List[str]
    suggestions: List[str]
    keywords: List[str]

def extract_skills(text: str) -> List[str]:
    """Extract skills from text using NER and skill database"""
    text_lower = text.lower()
    found_skills = set()
    
    for category, skills in SKILLS_DATABASE.items():
        for skill in skills:
            if skill.lower() in text_lower:
                found_skills.add(skill.title())
    
    # Use spaCy NER for additional extraction
    doc = nlp(text.lower())
    for ent in doc.ents:
        if ent.label_ in ["ORG", "GPE"]:
            found_skills.add(ent.text.title())
    
    return list(found_skills)

def extract_keywords(text: str, top_n: int = 10) -> List[str]:
    """Extract important keywords using TF-IDF-like approach"""
    # Remove common stopwords
    stopwords = {
        'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for',
        'of', 'with', 'by', 'from', 'is', 'was', 'are', 'be', 'been', 'being',
        'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could',
        'should', 'may', 'might', 'must', 'can', 'this', 'that', 'these', 'those'
    }
    
    words = re.findall(r'\b\w+\b', text.lower())
    filtered = [w for w in words if w not in stopwords and len(w) > 3]
    
    # Simple frequency-based extraction
    from collections import Counter
    freq = Counter(filtered)
    return [word.title() for word, _ in freq.most_common(top_n)]

def calculate_similarity(resume_text: str, job_description: str) -> float:
    """Calculate cosine similarity using BERT embeddings"""
    resume_embedding = model.encode(resume_text, convert_to_tensor=True)
    jd_embedding = model.encode(job_description, convert_to_tensor=True)
    
    similarity = util.pytorch_cos_sim(resume_embedding, jd_embedding).item()
    return max(0, min(1, similarity))  # Clamp between 0 and 1

def generate_suggestions(matched_skills: List[str], missing_skills: List[str], similarity: float) -> List[str]:
    """Generate improvement suggestions"""
    suggestions = []
    
    if similarity < 0.5:
        suggestions.append("Consider restructuring your resume to better match the job description")
        suggestions.append("Use more specific keywords from the job posting")
    
    if missing_skills:
        suggestions.append(f"Add experience with: {', '.join(missing_skills[:3])}")
    
    if len(matched_skills) < 5:
        suggestions.append("Highlight more technical skills and competencies")
    
    suggestions.append("Ensure proper formatting with clear sections (Experience, Skills, Education)")
    suggestions.append("Use quantifiable achievements and metrics in your descriptions")
    
    return suggestions[:5]  # Top 5 suggestions

@app.get("/api/health")
async def health():
    return {"status": "ML service is running"}

@app.post("/api/analyze", response_model=AnalysisResponse)
async def analyze_resume(request: AnalysisRequest):
    """Analyze resume against job description using BERT + NER"""
    try:
        # Extract skills
        resume_skills = set(extract_skills(request.resume_text))
        jd_skills = set(extract_skills(request.job_description))
        
        # Calculate similarity
        similarity = calculate_similarity(request.resume_text, request.job_description)
        
        # Matched and missing skills
        matched_skills = list(resume_skills & jd_skills)
        missing_skills = list(jd_skills - resume_skills)
        
        # ATS Score (0-100)
        ats_score = round(similarity * 100, 2)
        
        # Match percentage
        if jd_skills:
            match_percentage = round((len(matched_skills) / len(jd_skills)) * 100, 2)
        else:
            match_percentage = ats_score
        
        # Keywords
        keywords = extract_keywords(request.resume_text)
        
        # Suggestions
        suggestions = generate_suggestions(matched_skills, missing_skills, similarity)
        
        return AnalysisResponse(
            ats_score=ats_score,
            match_percentage=match_percentage,
            matched_skills=matched_skills,
            missing_skills=missing_skills,
            suggestions=suggestions,
            keywords=keywords
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
